"""
å®Œæ•´çš„ Debug è¨“ç·´è…³æœ¬
- è©³ç´°ç›£æ§ SLE å’Œ WLE çš„é æ¸¬
- æª¢æŸ¥ logits åˆ†å¸ƒ
- é©—è­‰ SAC mask æ˜¯å¦æ­£ç¢º
- é¡åˆ¥åˆ†å¸ƒçµ±è¨ˆ
"""

import torch
import torch.nn.functional as F
from transformers import WhisperTokenizerFast, WhisperConfig
from transformers import get_linear_schedule_with_warmup
from whisper_emotion_model import WhisperForEmotionRecognition
from generate_multitask_targets import WhisperEmotionDataPreprocessor
from preprocess_common_voice import CommonVoicePreprocessor

from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm.auto import tqdm
import time
from pathlib import Path
from datasets import load_from_disk
from torch.amp import autocast
import itertools
import gc
from collections import Counter
import numpy as np

torch.multiprocessing.set_sharing_strategy('file_system')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"å°‡ä½¿ç”¨ {device} é€²è¡Œè¨“ç·´...")

BASE_MODEL_NAME = "openai/whisper-large-v2"
TOKENIZER_PATH = "./custom_whisper_tokenizer"
MODEL_PATH = "./my-whisper-emotion-model-resized"

# ============================================================================
# è¼‰å…¥ Tokenizer
# ============================================================================
print(f"æ­£åœ¨å¾ {TOKENIZER_PATH} è¼‰å…¥ Tokenizer...")
tokenizer = WhisperTokenizerFast.from_pretrained(TOKENIZER_PATH)

sle_token_ids_full = {e: id for e, id in tokenizer.get_added_vocab().items() if e.startswith("<|sle_")}
wle_token_ids_full = {e: id for e, id in tokenizer.get_added_vocab().items() if e.startswith("<|wle_")}

sle_token_ids = {
    token.replace("<|sle_", "").replace("|>", ""): token_id
    for token, token_id in sle_token_ids_full.items()
}
wle_token_ids = {
    token.replace("<|wle_", "").replace("|>", ""): token_id
    for token, token_id in wle_token_ids_full.items()
}

print(f"âœ“ SLE Token IDs: {sle_token_ids}")
print(f"âœ“ WLE Token IDs: {wle_token_ids}")

# å»ºç«‹åå‘æ˜ å°„
id_to_emotion = {v: k for k, v in {**sle_token_ids, **wle_token_ids}.items()}
sle_ids_set = set(sle_token_ids.values())
wle_ids_set = set(wle_token_ids.values())

# ============================================================================
# è¼‰å…¥æ¨¡å‹
# ============================================================================
print(f"\næ­£åœ¨å¾ {MODEL_PATH} è¼‰å…¥æ¨¡å‹...")
config = WhisperConfig.from_pretrained(MODEL_PATH)

model = WhisperForEmotionRecognition.from_pretrained(
    MODEL_PATH,
    config=config,
    sle_token_ids=sle_token_ids,
    wle_token_ids=wle_token_ids,
    torch_dtype=torch.bfloat16
)

# èª¿æ•´ embedding å¤§å°ï¼Œä½¿æ¨¡å‹æ”¯æ´æ–°çš„æƒ…ç·’ token
model.resize_token_embeddings(len(tokenizer))

# **å¾ˆé‡è¦ï¼šé‡æ–°åˆå§‹åŒ–æ–°å¢ tokens çš„ embedding**
with torch.no_grad():
    new_num_tokens = len(tokenizer)
    old_num_tokens = model.config.vocab_size
    if new_num_tokens > old_num_tokens:
        print(f"ğŸ”§ åˆå§‹åŒ–æ–°å¢çš„ {new_num_tokens - old_num_tokens} å€‹ token embedding")
        model.model.decoder.embed_tokens.weight[old_num_tokens:new_num_tokens].normal_(mean=0.0, std=0.02)
        model.model.encoder.embed_tokens.weight[old_num_tokens:new_num_tokens].normal_(mean=0.0, std=0.02)

model.config.vocab_size = len(tokenizer)
# èª¿æ•´ embedding å¤§å°ï¼Œä½¿æ¨¡å‹æ”¯æ´æ–°çš„æƒ…ç·’ token
model.resize_token_embeddings(len(tokenizer))

# **å¾ˆé‡è¦ï¼šé‡æ–°åˆå§‹åŒ–æ–°å¢ tokens çš„ embedding**
with torch.no_grad():
    new_num_tokens = len(tokenizer)
    old_num_tokens = model.config.vocab_size
    if new_num_tokens > old_num_tokens:
        print(f"ğŸ”§ åˆå§‹åŒ–æ–°å¢çš„ {new_num_tokens - old_num_tokens} å€‹ token embedding")
        model.model.decoder.embed_tokens.weight[old_num_tokens:new_num_tokens].normal_(mean=0.0, std=0.02)
        model.model.encoder.embed_tokens.weight[old_num_tokens:new_num_tokens].normal_(mean=0.0, std=0.02)

model.config.vocab_size = len(tokenizer)

model.to(device)
print("âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ")

# ============================================================================
# è©³ç´°çš„æ¨¡å‹ç‹€æ…‹æª¢æŸ¥
# ============================================================================
def check_model_training_state(model, optimizer, emotion_ids):
    """å…¨é¢æª¢æŸ¥æ¨¡å‹è¨“ç·´ç‹€æ…‹"""
    print("\n" + "="*70)
    print("æ¨¡å‹è¨“ç·´ç‹€æ…‹æª¢æŸ¥")
    print("="*70)
    
    # 1. Embedding å±¤
    embed_layer = model.model.decoder.embed_tokens
    print(f"\n1. Embedding å±¤:")
    print(f"   requires_grad: {embed_layer.weight.requires_grad}")
    print(f"   shape: {embed_layer.weight.shape}")
    print(f"   dtype: {embed_layer.weight.dtype}")
    
    # 2. æƒ…ç·’ tokens
    if emotion_ids:
        emotion_emb = embed_layer.weight[list(emotion_ids)]
        print(f"\n2. æƒ…ç·’ Tokens:")
        print(f"   æ•¸é‡: {len(emotion_ids)}")
        print(f"   requires_grad: {emotion_emb.requires_grad}")
        print(f"   ç¯„æ•¸: {emotion_emb.norm().item():.4f}")
        print(f"   å¹³å‡å€¼: {emotion_emb.mean().item():.4f}")
        print(f"   æ¨™æº–å·®: {emotion_emb.std().item():.4f}")
    
    # 3. å„ªåŒ–å™¨
    optimizer_param_ids = {id(p) for group in optimizer.param_groups for p in group['params']}
    embed_in_optimizer = id(embed_layer.weight) in optimizer_param_ids
    
    print(f"\n3. å„ªåŒ–å™¨:")
    print(f"   ç¸½åƒæ•¸çµ„: {len(optimizer.param_groups)}")
    print(f"   Embedding åœ¨å„ªåŒ–å™¨ä¸­: {embed_in_optimizer}")
    
    # 4. çµ±è¨ˆ
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    
    print(f"\n4. åƒæ•¸çµ±è¨ˆ:")
    print(f"   ç¸½åƒæ•¸: {total:,}")
    print(f"   å¯è¨“ç·´: {trainable:,} ({trainable/total*100:.1f}%)")
    
    # 5. å•é¡Œè¨ºæ–·
    issues = []
    if not embed_layer.weight.requires_grad:
        issues.append("âŒ Embedding å±¤ä¸å¯è¨“ç·´")
    if not embed_in_optimizer:
        issues.append("âŒ Embedding ä¸åœ¨å„ªåŒ–å™¨ä¸­")
    
    if issues:
        print(f"\nâš ï¸  ç™¼ç¾å•é¡Œ:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print(f"\nâœ“ ä¸€åˆ‡æ­£å¸¸")
    
    print("="*70 + "\n")
    
    return len(issues) == 0


# é…ç½® SAC mask
model.set_sac_mask_config(
    use_in_training=True,
    use_in_inference=False
)

print("\nâœ“ SAC Mask é…ç½®:")
print("  - è¨“ç·´æ™‚: WLE ä¸èƒ½çœ‹åˆ° SLE")
print("  - SLE å¯ä»¥çœ‹åˆ°éŸ³è¨Šç‰¹å¾µï¼ˆcross-attentionï¼‰")
print("  - æ¨ç†æ™‚: ä¸ä½¿ç”¨ mask")

# ============================================================================
# è¼‰å…¥è³‡æ–™é›†
# ============================================================================
print("\næ­£åœ¨è¼‰å…¥è³‡æ–™é›†...")

iemocap_preprocessor = WhisperEmotionDataPreprocessor(
    base_model=BASE_MODEL_NAME,
    save_tokenizer_path=TOKENIZER_PATH
)
iemocap_preprocessor.tokenizer = tokenizer

train_dataset_iemocap = load_from_disk("./iemocap_processed/processed_train")
val_dataset_iemocap = load_from_disk("./iemocap_processed/processed_val")

cv_preprocessor = CommonVoicePreprocessor(
    base_model=BASE_MODEL_NAME,
    cv_data_path="/home/ouo/whisper_emotion/workspace/commonvoice/cv-corpus-21.0-delta-2025-03-14/en"
)
cv_preprocessor.tokenizer = tokenizer

train_dataset_cv = load_from_disk("./cv_processed/train")
val_dataset_cv = load_from_disk("./cv_processed/val")

print(f"âœ“ IEMOCAP è¨“ç·´é›†: {len(train_dataset_iemocap)} æ¨£æœ¬")
print(f"âœ“ Common Voice è¨“ç·´é›†: {len(train_dataset_cv)} æ¨£æœ¬")

# ============================================================================
# è©³ç´°è³‡æ–™åˆ†æ
# ============================================================================
print("\n" + "="*70)
print("è©³ç´°è³‡æ–™åˆ†æ")
print("="*70)

def analyze_labels_in_batch(labels, tokenizer, sle_ids, wle_ids, id_to_emotion):
    """åˆ†æä¸€å€‹æ‰¹æ¬¡ä¸­çš„æ¨™ç±¤"""
    sle_labels = []
    wle_labels = []
    
    for b in range(labels.shape[0]):
        for i, label in enumerate(labels[b]):
            label_item = label.item()
            
            if label_item == -100:
                continue
            
            if label_item in sle_ids:
                emotion = id_to_emotion.get(label_item, 'unknown')
                sle_labels.append((b, i, label_item, emotion))
            
            elif label_item in wle_ids:
                emotion = id_to_emotion.get(label_item, 'unknown')
                wle_labels.append((b, i, label_item, emotion))
    
    return sle_labels, wle_labels

# åˆ†æå‰ 10 å€‹è¨“ç·´æ¨£æœ¬
print("\nåˆ†æå‰ 10 å€‹è¨“ç·´æ¨£æœ¬çš„æ¨™ç±¤åˆ†å¸ƒ...")
sle_dist = Counter()
wle_dist = Counter()

for i in range(min(10, len(train_dataset_iemocap))):
    labels = train_dataset_iemocap[i]['labels']
    
    for label in labels:
        if label == -100:
            continue
        
        if label in sle_ids_set:
            emotion = id_to_emotion.get(label, 'unknown')
            sle_dist[emotion] += 1
        elif label in wle_ids_set:
            emotion = id_to_emotion.get(label, 'unknown')
            wle_dist[emotion] += 1

print(f"\nSLE åˆ†å¸ƒ:")
for emotion in ['neutral', 'happy', 'sad', 'angry']:
    count = sle_dist[emotion]
    pct = count / sum(sle_dist.values()) * 100 if sle_dist else 0
    print(f"  {emotion:8s}: {count:3d} ({pct:5.1f}%)")

print(f"\nWLE åˆ†å¸ƒ:")
for emotion in ['neutral', 'happy', 'sad', 'angry']:
    count = wle_dist[emotion]
    pct = count / sum(wle_dist.values()) * 100 if wle_dist else 0
    print(f"  {emotion:8s}: {count:4d} ({pct:5.1f}%)")

if sum(wle_dist.values()) == 0:
    print("\nâš ï¸  è­¦å‘Šï¼šè¨“ç·´è³‡æ–™ä¸­æ²’æœ‰ WLE tokensï¼")
    print("   é€™æœƒå°è‡´æ¨¡å‹ç„¡æ³•å­¸ç¿’è©ç´šæƒ…ç·’")
else:
    print(f"\nâœ“ è³‡æ–™åŒ…å« WLE tokens")

# ============================================================================
# Debug å‡½æ•¸
# ============================================================================
def debug_logits_distribution(outputs, labels, sle_ids, wle_ids, id_to_emotion, step):
    """
    è©³ç´°åˆ†æ logits åˆ†å¸ƒ
    æª¢æŸ¥æ¨¡å‹æ˜¯å¦åå‘é æ¸¬æŸå€‹é¡åˆ¥
    """
    logits = outputs.logits  # [batch, seq, vocab]
    
    # æ‰¾å‡ºæ‰€æœ‰ SLE ä½ç½®
    sle_analysis = []
    
    for b in range(labels.shape[0]):
        for i, label in enumerate(labels[b]):
            label_item = label.item()
            
            if label_item in sle_ids:
                # å–å¾—é€™å€‹ä½ç½®çš„ logits
                pos_logits = logits[b, i, :]
                
                # è¨ˆç®—å„æƒ…ç·’çš„åˆ†æ•¸
                emotion_scores = {}
                for emotion, emo_id in sle_token_ids.items():
                    emotion_scores[emotion] = pos_logits[emo_id].item()
                
                # é æ¸¬çš„æƒ…ç·’
                pred_id = pos_logits.argmax().item()
                pred_emotion = id_to_emotion.get(pred_id, 'unknown')
                
                # çœŸå¯¦çš„æƒ…ç·’
                true_emotion = id_to_emotion.get(label_item, 'unknown')
                
                sle_analysis.append({
                    'batch': b,
                    'pos': i,
                    'true_emotion': true_emotion,
                    'true_id': label_item,
                    'pred_emotion': pred_emotion,
                    'pred_id': pred_id,
                    'scores': emotion_scores,
                    'correct': (pred_id == label_item)
                })
    
    if sle_analysis:
        print(f"\n{'='*70}")
        print(f"[Step {step}] SLE Logits è©³ç´°åˆ†æ")
        print(f"{'='*70}")
        
        # å°å‡ºå‰ 3 å€‹ SLE çš„è©³ç´°è³‡è¨Š
        for i, analysis in enumerate(sle_analysis[:3]):
            print(f"\nSLE #{i+1} (batch {analysis['batch']}, pos {analysis['pos']}):")
            print(f"  çœŸå¯¦: {analysis['true_emotion']} (ID: {analysis['true_id']})")
            print(f"  é æ¸¬: {analysis['pred_emotion']} (ID: {analysis['pred_id']}) {'âœ“' if analysis['correct'] else 'âœ—'}")
            print(f"  å„æƒ…ç·’çš„ logits:")
            for emotion in ['neutral', 'happy', 'sad', 'angry']:
                score = analysis['scores'].get(emotion, 0)
                marker = "  â†" if emotion == analysis['pred_emotion'] else ""
                print(f"    {emotion:8s}: {score:8.2f}{marker}")
        
        # çµ±è¨ˆæº–ç¢ºç‡
        correct_count = sum(1 for a in sle_analysis if a['correct'])
        total_count = len(sle_analysis)
        accuracy = correct_count / total_count * 100
        
        print(f"\næ‰¹æ¬¡çµ±è¨ˆ:")
        print(f"  SLE ç¸½æ•¸: {total_count}")
        print(f"  é æ¸¬æ­£ç¢º: {correct_count} ({accuracy:.1f}%)")
        
        # çµ±è¨ˆé æ¸¬åˆ†å¸ƒ
        pred_dist = Counter(a['pred_emotion'] for a in sle_analysis)
        true_dist = Counter(a['true_emotion'] for a in sle_analysis)
        
        print(f"\n  çœŸå¯¦åˆ†å¸ƒ: {dict(true_dist)}")
        print(f"  é æ¸¬åˆ†å¸ƒ: {dict(pred_dist)}")
        
        # æª¢æŸ¥æ˜¯å¦åªé æ¸¬ä¸€å€‹é¡åˆ¥
        if len(pred_dist) == 1:
            print(f"\n  âš ï¸  æ¨¡å‹åªé æ¸¬ {list(pred_dist.keys())[0]}ï¼")
        
        print(f"{'='*70}\n")
    
    return sle_analysis

def debug_batch_labels(batch, tokenizer, sle_ids, wle_ids, id_to_emotion, step):
    """
    æª¢æŸ¥æ‰¹æ¬¡ä¸­çš„æ¨™ç±¤æ˜¯å¦æ­£ç¢º
    """
    labels = batch["labels"]
    decoder_input_ids = batch["decoder_input_ids"]
    
    print(f"\n{'='*70}")
    print(f"[Step {step}] æ‰¹æ¬¡æ¨™ç±¤æª¢æŸ¥")
    print(f"{'='*70}")
    
    # åˆ†ææ¨™ç±¤
    sle_labels, wle_labels = analyze_labels_in_batch(
        labels, tokenizer, sle_ids, wle_ids, id_to_emotion
    )
    
    print(f"\næ¨™ç±¤çµ±è¨ˆ:")
    print(f"  SLE tokens: {len(sle_labels)}")
    print(f"  WLE tokens: {len(wle_labels)}")
    
    if sle_labels:
        print(f"\nå‰ 3 å€‹ SLE æ¨™ç±¤:")
        for b, i, label_id, emotion in sle_labels[:3]:
            print(f"  Batch {b}, Pos {i}: {emotion} (ID: {label_id})")
    
    if wle_labels:
        print(f"\nå‰ 3 å€‹ WLE æ¨™ç±¤:")
        for b, i, label_id, emotion in wle_labels[:3]:
            print(f"  Batch {b}, Pos {i}: {emotion} (ID: {label_id})")
    
    # æª¢æŸ¥ decoder_input_ids
    print(f"\nDecoder Input IDs (ç¬¬ä¸€å€‹æ¨£æœ¬å‰ 20 å€‹):")
    if decoder_input_ids.shape[1] >= 20:
        first_20 = decoder_input_ids[0, :20].tolist()
        decoded = tokenizer.decode(first_20, skip_special_tokens=False)
        print(f"  IDs: {first_20}")
        print(f"  Decoded: {decoded}")
    
    print(f"{'='*70}\n")

def check_gradient_flow(model, step):
    """
    æª¢æŸ¥æ¢¯åº¦æµå‹•
    ç¢ºä¿ SLE/WLE embeddings æœ‰æ¢¯åº¦
    """
    print(f"\n{'='*70}")
    print(f"[Step {step}] æ¢¯åº¦æµå‹•æª¢æŸ¥")
    print(f"{'='*70}")
    
    # æª¢æŸ¥ embedding å±¤çš„æ¢¯åº¦
    embed_tokens = model.model.decoder.embed_tokens
    
    if embed_tokens.weight.grad is not None:
        grad_norm = embed_tokens.weight.grad.norm().item()
        print(f"\nEmbedding æ¢¯åº¦ç¯„æ•¸: {grad_norm:.4f}")
        
        # æª¢æŸ¥ SLE/WLE tokens çš„æ¢¯åº¦
        all_emotion_ids = list(sle_ids_set | wle_ids_set)
        if all_emotion_ids:
            emotion_grads = embed_tokens.weight.grad[all_emotion_ids]
            emotion_grad_norm = emotion_grads.norm().item()
            print(f"æƒ…ç·’ token æ¢¯åº¦ç¯„æ•¸: {emotion_grad_norm:.4f}")
            
            if emotion_grad_norm < 1e-6:
                print(f"  âš ï¸  æƒ…ç·’ token çš„æ¢¯åº¦éå¸¸å°ï¼")
    else:
        print("\n  âš ï¸  Embedding å±¤æ²’æœ‰æ¢¯åº¦ï¼")
    
    print(f"{'='*70}\n")

# ============================================================================
# è¨“ç·´åƒæ•¸
# ============================================================================
BATCH_SIZE = 2
ACCUMULATION_STEPS = 1
NUM_WORKERS = 0
WARMUP_STEPS = 500
LEARNING_RATE = 1e-5
EPOCHS = 2
MODEL_SAVE_PATH = Path("./whisper_emotion_debug")
MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)

BETA = 0.5
VALIDATION_STEPS = 250
DEBUG_INTERVAL = 50  # æ¯ 50 æ­¥è©³ç´° debug

global_step_count = 0
best_val_loss = float("inf")

print(f"\nè¨“ç·´é…ç½®:")
print(f"  - æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
print(f"  - ç´¯ç©æ­¥æ•¸: {ACCUMULATION_STEPS}")
print(f"  - å­¸ç¿’ç‡: {LEARNING_RATE}")
print(f"  - Epochs: {EPOCHS}")
print(f"  - Beta: {BETA}")
print(f"  - Debug é–“éš”: æ¯ {DEBUG_INTERVAL} æ­¥")
print(f"  - æ¨¡å‹å„²å­˜: {MODEL_SAVE_PATH}")

# ============================================================================
# DataLoader
# ============================================================================
loader_iemocap = iemocap_preprocessor.create_dataloader(
    train_dataset_iemocap, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS
)
loader_cv = cv_preprocessor.create_dataloader(
    train_dataset_cv, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS
)

val_loader_iemocap = iemocap_preprocessor.create_dataloader(
    val_dataset_iemocap, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS
)
val_loader_cv = cv_preprocessor.create_dataloader(
    val_dataset_cv, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS
)

# ============================================================================
# å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨
# ============================================================================
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)


# åœ¨å»ºç«‹å„ªåŒ–å™¨ä¹‹å¾Œèª¿ç”¨
all_emotion_ids = sle_ids_set | wle_ids_set
is_healthy = check_model_training_state(model, optimizer, all_emotion_ids)


if not is_healthy:
    print("ç™¼ç¾å•é¡Œï¼Œéœ€è¦ä¿®æ­£å¾Œå†é–‹å§‹è¨“ç·´ï¼")
    # é€™è£¡å¯ä»¥åŠ å…¥ä¿®æ­£ä»£ç¢¼


num_training_steps = len(loader_iemocap) * EPOCHS // ACCUMULATION_STEPS
warmup_scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=WARMUP_STEPS,
    num_training_steps=num_training_steps
)

plateau_scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='min', 
    factor=0.5, 
    patience=2, 
    verbose=True
)

# ============================================================================
# è¨“ç·´è¿´åœˆ
# ============================================================================
print("\n" + "="*70)
print("é–‹å§‹è¨“ç·´ï¼ˆDebug æ¨¡å¼ï¼‰")
print("="*70 + "\n")

# ç”¨æ–¼è¿½è¹¤è¨“ç·´éç¨‹ä¸­çš„çµ±è¨ˆ
training_stats = {
    'sle_correct': [],
    'sle_total': [],
    'pred_distribution': Counter(),
    'true_distribution': Counter()
}

for epoch in range(EPOCHS):
    start_time = time.time()
    model.train()
    
    total_train_loss = 0
    total_loss_iemocap = 0
    total_loss_cv = 0
    num_batches = 0
    
    # å»ºç«‹è¨“ç·´è¿­ä»£å™¨
    if len(loader_iemocap) > len(loader_cv):
        train_progress_bar = tqdm(loader_iemocap, desc=f"Epoch {epoch+1}/{EPOCHS}")
        cv_iter = itertools.cycle(loader_cv)
        main_loader = loader_iemocap
        is_cv_main = False
    else:
        train_progress_bar = tqdm(loader_cv, desc=f"Epoch {epoch+1}/{EPOCHS}")
        iemocap_iter = itertools.cycle(loader_iemocap)
        main_loader = loader_cv
        is_cv_main = True

    optimizer.zero_grad()
    
    for i, main_batch in enumerate(train_progress_bar):
        try:
            # å–å¾—å…©å€‹è³‡æ–™é›†çš„æ‰¹æ¬¡
            if is_cv_main:
                batch_cv = main_batch
                batch_iemocap = next(iemocap_iter)
            else:
                batch_iemocap = main_batch
                batch_cv = next(cv_iter)

            # ============================================================
            # IEMOCAP å‰å‘å‚³æ’­
            # ============================================================
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs_iemocap = model(
                    input_features=batch_iemocap["input_features"].to(device),
                    labels=batch_iemocap["labels"].to(device),
                    decoder_input_ids=batch_iemocap["decoder_input_ids"].to(device)
                )
                loss_iemocap = outputs_iemocap.loss
            
            # ============================================================
            # Common Voice å‰å‘å‚³æ’­
            # ============================================================
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs_cv = model(
                    input_features=batch_cv["input_features"].to(device),
                    labels=batch_cv["labels"].to(device),
                    decoder_input_ids=batch_cv["decoder_input_ids"].to(device)
                )
                loss_cv = outputs_cv.loss
            
            # çµ„åˆ loss
            loss = loss_iemocap + (BETA * loss_cv)
            
            # ============================================================
            # Debug è¼¸å‡º
            # ============================================================
            if global_step_count % DEBUG_INTERVAL == 0:
                # 1. æª¢æŸ¥æ‰¹æ¬¡æ¨™ç±¤
                debug_batch_labels(
                    batch_iemocap,
                    tokenizer,
                    sle_ids_set,
                    wle_ids_set,
                    id_to_emotion,
                    global_step_count
                )
                
                # 2. åˆ†æ logits åˆ†å¸ƒ
                sle_analysis = debug_logits_distribution(
                    outputs_iemocap,
                    batch_iemocap["labels"].to(device),
                    sle_ids_set,
                    wle_ids_set,
                    id_to_emotion,
                    global_step_count
                )
                
                # æ›´æ–°çµ±è¨ˆ
                if sle_analysis:
                    correct = sum(1 for a in sle_analysis if a['correct'])
                    total = len(sle_analysis)
                    training_stats['sle_correct'].append(correct)
                    training_stats['sle_total'].append(total)
                    
                    for a in sle_analysis:
                        training_stats['pred_distribution'][a['pred_emotion']] += 1
                        training_stats['true_distribution'][a['true_emotion']] += 1
            
            # ============================================================
            # æ¯ 200 æ­¥æª¢æŸ¥æ¢¯åº¦
            # ============================================================
            if global_step_count % 200 == 0 and global_step_count > 0:
                check_gradient_flow(model, global_step_count)
            
            # ============================================================
            # åå‘å‚³æ’­
            # ============================================================
            if ACCUMULATION_STEPS > 1:
                loss = loss / ACCUMULATION_STEPS
            
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"\n!! NaN/Inf Lossï¼Œè·³é !!")
                optimizer.zero_grad()
                continue
            
            loss.backward()
            
            total_train_loss += loss.item() * ACCUMULATION_STEPS
            total_loss_iemocap += loss_iemocap.item()
            total_loss_cv += loss_cv.item()
            num_batches += 1

            # ============================================================
            # å„ªåŒ–å™¨æ­¥é©Ÿ
            # ============================================================
            if (i + 1) % ACCUMULATION_STEPS == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                warmup_scheduler.step()
                optimizer.zero_grad()
                
                train_progress_bar.set_postfix(
                    total=loss.item() * ACCUMULATION_STEPS, 
                    iem=loss_iemocap.item(),
                    cv=loss_cv.item()
                )
                
                global_step_count += 1
                
                # ========================================================
                # å°å‡ºè¨“ç·´çµ±è¨ˆ
                # ========================================================
                if global_step_count % 100 == 0:
                    if training_stats['sle_total']:
                        recent_correct = sum(training_stats['sle_correct'][-10:])
                        recent_total = sum(training_stats['sle_total'][-10:])
                        recent_acc = recent_correct / max(recent_total, 1) * 100
                        
                        print(f"\n[Step {global_step_count}] æœ€è¿‘ 10 æ¬¡çµ±è¨ˆ:")
                        print(f"  SLE æº–ç¢ºç‡: {recent_acc:.1f}% ({recent_correct}/{recent_total})")
                        print(f"  çœŸå¯¦åˆ†å¸ƒ: {dict(training_stats['true_distribution'])}")
                        print(f"  é æ¸¬åˆ†å¸ƒ: {dict(training_stats['pred_distribution'])}")
                
                # ========================================================
                # é©—è­‰
                # ========================================================
                if global_step_count % VALIDATION_STEPS == 0:
                    model.eval()
                    print(f"\n{'='*70}")
                    print(f"é©—è­‰ (Step {global_step_count})")
                    print(f"{'='*70}")
                    
                    # IEMOCAP é©—è­‰
                    total_iem_val = 0
                    iem_val_batches = 0
                    val_sle_correct = 0
                    val_sle_total = 0
                    val_pred_dist = Counter()
                    val_true_dist = Counter()
                    
                    with torch.no_grad():
                        for val_batch in tqdm(val_loader_iemocap, desc="é©—è­‰ IEMOCAP", leave=False):
                            try:
                                with autocast(device_type='cuda', dtype=torch.bfloat16):
                                    val_out = model(
                                        input_features=val_batch["input_features"].to(device),
                                        labels=val_batch["labels"].to(device),
                                        decoder_input_ids=val_batch["decoder_input_ids"].to(device)
                                    )
                                    total_iem_val += val_out.loss.item()
                                    iem_val_batches += 1
                                    
                                    # åˆ†æé©—è­‰æ‰¹æ¬¡
                                    val_analysis = debug_logits_distribution(
                                        val_out,
                                        val_batch["labels"].to(device),
                                        sle_ids_set,
                                        wle_ids_set,
                                        id_to_emotion,
                                        global_step_count
                                    )
                                    
                                    if val_analysis:
                                        val_sle_correct += sum(1 for a in val_analysis if a['correct'])
                                        val_sle_total += len(val_analysis)
                                        
                                        for a in val_analysis:
                                            val_pred_dist[a['pred_emotion']] += 1
                                            val_true_dist[a['true_emotion']] += 1
                                    
                            except Exception as e:
                                print(f"é©—è­‰æ‰¹æ¬¡éŒ¯èª¤: {e}")
                                continue
                    
                    avg_iem_val = total_iem_val / max(iem_val_batches, 1)
                    val_sle_acc = val_sle_correct / max(val_sle_total, 1) * 100
                    
                    # CV é©—è­‰
                    total_cv_val = 0
                    cv_val_batches = 0
                    
                    with torch.no_grad():
                        for val_batch in tqdm(val_loader_cv, desc="é©—è­‰ CV", leave=False):
                            try:
                                with autocast(device_type='cuda', dtype=torch.bfloat16):
                                    val_out = model(
                                        input_features=val_batch["input_features"].to(device),
                                        labels=val_batch["labels"].to(device),
                                        decoder_input_ids=val_batch["decoder_input_ids"].to(device)
                                    )
                                    total_cv_val += val_out.loss.item()
                                    cv_val_batches += 1
                            except Exception as e:
                                continue
                    
                    avg_cv_val = total_cv_val / max(cv_val_batches, 1)
                    
                    print(f"\né©—è­‰çµæœ:")
                    print(f"  IEMOCAP Loss: {avg_iem_val:.4f}")
                    print(f"  CV Loss:      {avg_cv_val:.4f}")
                    print(f"  SLE æº–ç¢ºç‡:   {val_sle_acc:.1f}% ({val_sle_correct}/{val_sle_total})")
                    
                    print(f"\né©—è­‰é›†æƒ…ç·’åˆ†å¸ƒ:")
                    print(f"  çœŸå¯¦: {dict(val_true_dist)}")
                    print(f"  é æ¸¬: {dict(val_pred_dist)}")
                    
                    # æª¢æŸ¥æ˜¯å¦åªé æ¸¬ä¸€å€‹é¡åˆ¥
                    if val_pred_dist:
                        dominant_emotion = val_pred_dist.most_common(1)[0]
                        dominant_ratio = dominant_emotion[1] / sum(val_pred_dist.values())
                        if dominant_ratio > 0.9:
                            print(f"\n  âš ï¸  é©—è­‰é›†æœ‰ {dominant_ratio*100:.1f}% é æ¸¬ç‚º {dominant_emotion[0]}")
                    
                    plateau_scheduler.step(avg_iem_val)
                    
                    if avg_iem_val < best_val_loss:
                        best_val_loss = avg_iem_val
                        print(f"\nğŸ¯ æœ€ä½³æ¨¡å‹ï¼å„²å­˜è‡³ {MODEL_SAVE_PATH}...")
                        model.save_pretrained(MODEL_SAVE_PATH)
                        tokenizer.save_pretrained(MODEL_SAVE_PATH)
                        
                        # å„²å­˜çµ±è¨ˆè³‡è¨Š
                        import json
                        stats_file = MODEL_SAVE_PATH / "training_stats.json"
                        with open(stats_file, 'w') as f:
                            json.dump({
                                'val_loss': avg_iem_val,
                                'val_accuracy': val_sle_acc,
                                'pred_distribution': dict(val_pred_dist),
                                'true_distribution': dict(val_true_dist),
                                'step': global_step_count
                            }, f, indent=2)
                        
                        print("âœ“ å·²å„²å­˜")
                    
                    print(f"{'='*70}\n")
                    model.train()
                    
                    gc.collect()
                    torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"\nè¨“ç·´æ‰¹æ¬¡éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            optimizer.zero_grad()
            continue

    # Epoch çµæŸ
    avg_train_loss = total_train_loss / max(num_batches, 1)
    avg_iem_loss = total_loss_iemocap / max(num_batches, 1)
    avg_cv_loss = total_loss_cv / max(num_batches, 1)
    
    epoch_time = (time.time() - start_time) / 60
    
    # è¨ˆç®— epoch çµ±è¨ˆ
    if training_stats['sle_total']:
        epoch_correct = sum(training_stats['sle_correct'])
        epoch_total = sum(training_stats['sle_total'])
        epoch_acc = epoch_correct / max(epoch_total, 1) * 100
    else:
        epoch_acc = 0
    
    print("\n" + "="*70)
    print(f"Epoch {epoch+1}/{EPOCHS} å®Œæˆ ({epoch_time:.1f} åˆ†é˜)")
    print("="*70)
    print(f"  ç¸½ Loss:      {avg_train_loss:.4f}")
    print(f"  IEM Loss:     {avg_iem_loss:.4f}")
    print(f"  CV Loss:      {avg_cv_loss:.4f}")
    print(f"  SLE æº–ç¢ºç‡:   {epoch_acc:.1f}%")
    print(f"  æœ€ä½³ Val:     {best_val_loss:.4f}")
    print("="*70 + "\n")
    
    # é‡ç½® epoch çµ±è¨ˆ
    training_stats = {
        'sle_correct': [],
        'sle_total': [],
        'pred_distribution': Counter(),
        'true_distribution': Counter()
    }
    
    gc.collect()
    torch.cuda.empty_cache()

print("\n" + "="*70)
print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
print(f"æœ€ä½³æ¨¡å‹å·²å„²å­˜è‡³: {MODEL_SAVE_PATH}")
print("="*70)
