"""
å®Œæ•´çš„è¨“ç·´è…³æœ¬ - Epoch çµæŸè©•ä¼°ç‰ˆ
- æ¯å€‹ epoch çµæŸå¾Œè©•ä¼° WER, UAR, WA
- ä½¿ç”¨ UAR ä½œç‚ºæœ€ä½³æ¨¡å‹åˆ¤æ–·æ¨™æº–ï¼ˆé©åˆä¸å¹³è¡¡è³‡æ–™ï¼‰
- ä½¿ç”¨ bf16 æ··åˆç²¾åº¦è¨“ç·´
- çµæœä¿å­˜ç‚º JSON æª”æ¡ˆ
"""

import torch
import torch.nn.functional as F
from transformers import WhisperTokenizerFast, WhisperConfig
from transformers import get_linear_schedule_with_warmup
from whisper_emotion_model import WhisperForEmotionRecognition
from generate_multitask_targets import WhisperEmotionDataPreprocessor
from preprocess_common_voice import CommonVoicePreprocessorForRehearsal

from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm.auto import tqdm
import time
from pathlib import Path
from datasets import load_from_disk
from torch.amp import autocast
import itertools
import gc
from collections import Counter
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, confusion_matrix
import evaluate
import json
from datetime import datetime

torch.multiprocessing.set_sharing_strategy('file_system')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"å°‡ä½¿ç”¨ {device} é€²è¡Œè¨“ç·´...")

BASE_MODEL_NAME = "openai/whisper-base"
TOKENIZER_PATH = "./custom_whisper_tokenizer"
MODEL_PATH = "./my-whisper-emotion-model-reinit-v2"

# ============================================================================
# è¼‰å…¥ Tokenizer
# ============================================================================
print(f"æ­£åœ¨å¾ {TOKENIZER_PATH} è¼‰å…¥ Tokenizer...")
tokenizer = WhisperTokenizerFast.from_pretrained(TOKENIZER_PATH)

sle_token_ids_full = {e: id for e, id in tokenizer.get_added_vocab().items() if e.startswith("<|sle_")}
wle_token_ids_full = {e: id for e, id in tokenizer.get_added_vocab().items() if e.startswith("<|wle_")}

sle_token_ids = {k.replace("<|sle_", "").replace("|>", ""): v for k, v in sle_token_ids_full.items()}
wle_token_ids = {k.replace("<|wle_", "").replace("|>", ""): v for k, v in wle_token_ids_full.items()}

print(f"âœ“ SLE Token IDs: {sle_token_ids}")
print(f"âœ“ WLE Token IDs: {wle_token_ids}")

# å»ºç«‹åå‘æ˜ å°„
id_to_emotion = {}
for emo, tid in sle_token_ids.items():
    id_to_emotion[tid] = emo
for emo, tid in wle_token_ids.items():
    id_to_emotion[tid] = emo

sle_ids_set = set(sle_token_ids.values())
wle_ids_set = set(wle_token_ids.values())

# ============================================================================
# è¼‰å…¥æ¨¡å‹
# ============================================================================
print(f"\næ­£åœ¨å¾ {MODEL_PATH} è¼‰å…¥æ¨¡å‹...")
config = WhisperConfig.from_pretrained(MODEL_PATH)

model = WhisperForEmotionRecognition.from_pretrained(
    MODEL_PATH,
    config=config,
    sle_token_ids=sle_token_ids,
    wle_token_ids=wle_token_ids,
    torch_dtype=torch.bfloat16  #ç”¨BF16
)

# èª¿æ•´ä¸¦æ­£ç¢ºåˆå§‹åŒ–æ–°å¢ token çš„ embedding
new_num_tokens = len(tokenizer)
old_num_tokens = model.config.vocab_size

if new_num_tokens != old_num_tokens:
    model.resize_token_embeddings(new_num_tokens)
    if new_num_tokens > old_num_tokens:
        with torch.no_grad():
            print(f"ğŸ”§ åˆå§‹åŒ–æ–°å¢çš„ {new_num_tokens - old_num_tokens} å€‹ token embedding")
            model.model.decoder.embed_tokens.weight[old_num_tokens:new_num_tokens].normal_(mean=0.0, std=0.02)
            try:
                model.model.encoder.embed_tokens.weight[old_num_tokens:new_num_tokens].normal_(mean=0.0, std=0.02)
            except Exception:
                pass

model.config.vocab_size = new_num_tokens
model.to(device)
print("âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ (ä½¿ç”¨ bf16)")

# é…ç½® SAC mask
model.set_sac_mask_config(
    use_in_training=True,
    use_in_inference=False
)

print("\nâœ“ SAC Mask é…ç½®:")
print("  - è¨“ç·´æ™‚: WLE ä¸èƒ½çœ‹åˆ° SLE")
print("  - æ¨ç†æ™‚: ä¸ä½¿ç”¨ mask")

# ============================================================================
# è¼‰å…¥è³‡æ–™é›†
# ============================================================================
print("\næ­£åœ¨è¼‰å…¥è³‡æ–™é›†...")

iemocap_preprocessor = WhisperEmotionDataPreprocessor(
    base_model=BASE_MODEL_NAME,
    save_tokenizer_path=TOKENIZER_PATH
)
iemocap_preprocessor.tokenizer = tokenizer

train_dataset_iemocap = load_from_disk("./iemocap_processed/processed_train")
val_dataset_iemocap = load_from_disk("./iemocap_processed/processed_val")

cv_preprocessor = CommonVoicePreprocessorForRehearsal(
    base_model=BASE_MODEL_NAME,
    cv_data_path="/home/ouo/whisper_emotion/workspace/CV/en/clips",
    custom_tokenizer_path=TOKENIZER_PATH
)
cv_preprocessor.tokenizer = tokenizer

train_dataset_cv = load_from_disk("./cv_processed_for_rehearsal/train")
val_dataset_cv = load_from_disk("./cv_processed_for_rehearsal/val")

print(f"âœ“ IEMOCAP è¨“ç·´é›†: {len(train_dataset_iemocap)} æ¨£æœ¬")
print(f"âœ“ IEMOCAP é©—è­‰é›†: {len(val_dataset_iemocap)} æ¨£æœ¬")
print(f"âœ“ Common Voice è¨“ç·´é›†: {len(train_dataset_cv)} æ¨£æœ¬")
print(f"âœ“ Common Voice é©—è­‰é›†: {len(val_dataset_cv)} æ¨£æœ¬")

# ============================================================================
# è¨“ç·´åƒæ•¸
# ============================================================================
BATCH_SIZE = 1# è«–æ–‡ä½¿ç”¨ batch_size=4
ACCUMULATION_STEPS = 4 #æœ‰æ•ˆbatchsize=4
NUM_WORKERS = 4
WARMUP_STEPS = 0  # è«–æ–‡æœªæåŠ warmup
LEARNING_RATE = 1e-5  # è«–æ–‡æœªæŒ‡å®šï¼Œä¿æŒé è¨­
EPOCHS = 2  # è«–æ–‡è¨“ç·´ 2 epochs
MODEL_SAVE_PATH = Path("./whisper_emotion_bf16")
MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)

BETA = 0.5
VAL_BATCH_SIZE = 4  # é©—è­‰æ™‚å¯ä»¥ç”¨è¼ƒå¤§çš„ batch

# è‡ªå‹•è¨ˆç®—é©—è­‰é »ç‡ï¼ˆ1/32 epochï¼‰
# è«–æ–‡è¨­å®š: batch_size=4
# IEMOCAP ç´„ 4000 æ¨£æœ¬ï¼Œæ¯å€‹ epoch = 4000/4 = 1000 æ­¥
STEPS_PER_EPOCH = len(train_dataset_iemocap) // BATCH_SIZE
VALIDATION_STEPS = max(STEPS_PER_EPOCH // 32, 1)  # 1/32 epochï¼Œæœ€å°‘ 1 æ­¥

global_step_count = 0
best_uar = 0.0  # æ”¹ç”¨ UAR ä½œç‚ºæœ€ä½³æ¨¡å‹åˆ¤æ–·æ¨™æº–

print(f"\né©—è­‰é »ç‡è¨ˆç®—:")
print(f"  é ä¼°æ¯ epoch æ­¥æ•¸: {STEPS_PER_EPOCH}")
print(f"  é©—è­‰é–“éš”: æ¯ {VALIDATION_STEPS} æ­¥ (1/32 epoch)")
print(f"  æ¯ epoch é©—è­‰æ¬¡æ•¸: ~{STEPS_PER_EPOCH // VALIDATION_STEPS} æ¬¡")

print(f"\nè¨“ç·´é…ç½® (ä¾æ“šè«–æ–‡è¨­å®š):")
print(f"  - æ¨¡å‹: whisper-base (è«–æ–‡ç”¨ large-v2)")
print(f"  - æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} (è«–æ–‡è¨­å®š)")
print(f"  - ç´¯ç©æ­¥æ•¸: {ACCUMULATION_STEPS}")
print(f"  - å­¸ç¿’ç‡: {LEARNING_RATE}")
print(f"  - Epochs: {EPOCHS} (è«–æ–‡è¨­å®š)")
print(f"  - Beta: {BETA}")
print(f"  - é©—è­‰æ‰¹æ¬¡: {VAL_BATCH_SIZE}")
print(f"  - æ¨¡å‹å„²å­˜: {MODEL_SAVE_PATH}")
print(f"  - ç²¾åº¦: BF16")
print(f"  - æœ€ä½³æ¨¡å‹æ¨™æº–: UAR (é©åˆä¸å¹³è¡¡è³‡æ–™)")

# ============================================================================
# DataLoader
# ============================================================================
loader_iemocap = iemocap_preprocessor.create_dataloader(
    train_dataset_iemocap, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS
)
loader_cv = cv_preprocessor.create_dataloader(
    train_dataset_cv, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS
)

val_loader_iemocap = iemocap_preprocessor.create_dataloader(
    val_dataset_iemocap, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS
)
val_loader_cv = cv_preprocessor.create_dataloader(
    val_dataset_cv, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS
)

# ============================================================================
# å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨ï¼ˆä¾æ“šè«–æ–‡ï¼‰
# ============================================================================
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)

# è«–æ–‡ä½¿ç”¨ NewBobScheduler (ReduceLROnPlateau çš„è®Šé«”)
# é€™è£¡ä½¿ç”¨æ¨™æº–çš„ ReduceLROnPlateau
plateau_scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='max',  # UAR è¶Šé«˜è¶Šå¥½
    factor=0.5,  # LR é™ä½ 50%
    patience=2,  # 2 æ¬¡é©—è­‰ç„¡æ”¹å–„æ‰é™ä½
    verbose=True,
    min_lr=1e-7  # æœ€å°å­¸ç¿’ç‡
)

# ä¸ä½¿ç”¨ warmup scheduler (è«–æ–‡æœªæåŠ)

#scaler = torch.cuda.amp.GradScaler()

# ============================================================================
# å¿«é€Ÿé©—è­‰å‡½æ•¸ï¼ˆåªè¨ˆç®— loss å’Œ UARï¼‰
# ============================================================================
def quick_validation(model, val_loader_iemocap, device, sle_token_ids, id_to_emotion, step):
    """
    å¿«é€Ÿé©—è­‰ï¼šåªè¨ˆç®— loss å’Œ UARï¼ˆè·³é WERï¼‰
    ç”¨æ–¼è¨“ç·´éç¨‹ä¸­çš„å³æ™‚ç›£æ§
    """
    model.eval()
    
    total_val_loss = 0
    val_batches = 0
    all_true_sle_ids = []
    all_pred_sle_ids = []
    
    sle_id_list = list(sle_token_ids.values())
    vocab_size = model.config.vocab_size
    ser_mask = torch.full((vocab_size,), float('-inf'), device=device)
    for token_id in sle_id_list:
        if token_id is not None and 0 <= token_id < vocab_size:
            ser_mask[token_id] = 0.0
    
    iemocap_prompt_len = 3
    iemocap_true_sle_idx = 3
    
    with torch.no_grad():
        # åªå–éƒ¨åˆ†æ¨£æœ¬åŠ é€Ÿï¼ˆæœ€å¤š 200 å€‹æ¨£æœ¬ï¼‰
        val_samples = 0
        max_val_samples = 200
        
        for batch in val_loader_iemocap:
            if val_samples >= max_val_samples:
                break
                
            input_features = batch["input_features"].to(device)
            labels = batch["labels"].to(device)
            true_decoder_ids = batch["decoder_input_ids"].to(device)
            batch_size = input_features.shape[0]
            
            # è¨ˆç®— loss
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs = model(
                    input_features=input_features,
                    labels=labels,
                    decoder_input_ids=true_decoder_ids
                )
                total_val_loss += outputs.loss.item()
                val_batches += 1
            
            # è¨ˆç®— SER
            true_sle_ids_batch = true_decoder_ids[:, iemocap_true_sle_idx].cpu().numpy()
            decoder_input_for_ser = true_decoder_ids[:, :iemocap_prompt_len]
            
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                ser_outputs = model(
                    input_features=input_features,
                    decoder_input_ids=decoder_input_for_ser,
                    labels=None,
                )
                next_token_logits = ser_outputs.logits[:, -1, :]
                masked_logits = next_token_logits + ser_mask
                pred_sle_ids_batch = torch.argmax(masked_logits, dim=-1).cpu().numpy()
            
            all_true_sle_ids.extend(true_sle_ids_batch.tolist())
            all_pred_sle_ids.extend(pred_sle_ids_batch.tolist())
            
            val_samples += batch_size
    
    avg_val_loss = total_val_loss / max(val_batches, 1)
    
    # è¨ˆç®— UAR
    uar = recall_score(
        all_true_sle_ids, all_pred_sle_ids,
        average='macro', labels=sle_id_list, zero_division=0
    )
    
    # è¨ˆç®— WA
    wa = accuracy_score(all_true_sle_ids, all_pred_sle_ids)
    
    # æ¯é¡åˆ¥å¬å›ç‡
    all_true_emotions = [id_to_emotion.get(id, 'unknown') for id in all_true_sle_ids]
    all_pred_emotions = [id_to_emotion.get(id, 'unknown') for id in all_pred_sle_ids]
    emotions = ['neutral', 'happy', 'sad', 'angry']
    cm = confusion_matrix(all_true_emotions, all_pred_emotions, labels=emotions)
    
    per_class_recall = {}
    for i, emotion in enumerate(emotions):
        recall = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0
        per_class_recall[emotion] = float(recall)
    
    print(f"\n{'='*70}")
    print(f"[Step {step}] å¿«é€Ÿé©—è­‰ï¼ˆ{val_samples} æ¨£æœ¬ï¼‰")
    print(f"{'='*70}")
    print(f"  Loss: {avg_val_loss:.4f}")
    print(f"  UAR:  {uar * 100:.2f}%")
    print(f"  WA:   {wa * 100:.2f}%")
    print(f"  æ¯é¡åˆ¥å¬å›ç‡:")
    for emotion in emotions:
        recall_pct = per_class_recall[emotion] * 100
        print(f"    {emotion:8s}: {recall_pct:6.2f}%")
    print(f"{'='*70}\n")
    
    model.train()
    
    return avg_val_loss, uar, wa


# ============================================================================
# è©•ä¼°å‡½æ•¸
# ============================================================================
def evaluate_model(model, val_loader_iemocap, val_loader_cv, tokenizer, device, 
                   sle_token_ids, wle_token_ids, id_to_emotion, epoch):
    """
    å®Œæ•´è©•ä¼°å‡½æ•¸ï¼šè¨ˆç®— WER, UAR, WA
    """
    print(f"\n{'='*70}")
    print(f"Epoch {epoch} å®Œæ•´è©•ä¼°")
    print(f"{'='*70}")
    
    model.eval()
    
    # æº–å‚™è©•ä¼°æŒ‡æ¨™
    wer_metric_iemocap = evaluate.load("wer")
    wer_metric_cv = evaluate.load("wer")
    
    all_true_sle_ids = []
    all_pred_sle_ids = []
    all_references_iemocap = []
    all_predictions_iemocap = []
    all_references_cv = []
    all_predictions_cv = []
    
    sle_id_list = list(sle_token_ids.values())
    wle_id_list = list(wle_token_ids.values())
    
    # å‰µå»º SER é®ç½©
    vocab_size = model.config.vocab_size
    ser_mask = torch.full((vocab_size,), float('-inf'), device=device)
    for token_id in sle_id_list:
        if token_id is not None and 0 <= token_id < vocab_size:
            ser_mask[token_id] = 0.0
    
    # IEMOCAP æ ¼å¼åƒæ•¸
    iemocap_prompt_len = 3  # [SOT, EN, TRANSCRIBE]
    iemocap_true_sle_idx = 3
    
    # ========================================================================
    # è©•ä¼° IEMOCAP (SER + ASR)
    # ========================================================================
    print("\nè©•ä¼° IEMOCAP...")
    
    with torch.no_grad():
        for batch in tqdm(val_loader_iemocap, desc="IEMOCAP é©—è­‰"):
            input_features = batch["input_features"].to(device)
            true_decoder_ids = batch["decoder_input_ids"].to(device)
            references = batch["original_texts"]
            batch_size = input_features.shape[0]
            
            # --- SER è©•ä¼° ---
            true_sle_ids_batch = true_decoder_ids[:, iemocap_true_sle_idx].cpu().numpy()
            decoder_input_for_ser = true_decoder_ids[:, :iemocap_prompt_len]
            
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs = model(
                    input_features=input_features,
                    decoder_input_ids=decoder_input_for_ser,
                    labels=None,
                )
                next_token_logits = outputs.logits[:, -1, :]
                masked_logits = next_token_logits + ser_mask
                pred_sle_ids_batch = torch.argmax(masked_logits, dim=-1).cpu().numpy()
            
            all_true_sle_ids.extend(true_sle_ids_batch.tolist())
            all_pred_sle_ids.extend(pred_sle_ids_batch.tolist())
            
            # --- ASR è©•ä¼° ---
            decoder_prompt_asr = true_decoder_ids[:, :4]  # [SOT, EN, TRANSCRIBE, SLE]
            suppress_token_list = wle_id_list
            
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                predicted_ids_asr = model.generate(
                    input_features=input_features,
                    decoder_input_ids=decoder_prompt_asr,
                    max_length=448,
                    use_cache=False,
                    suppress_tokens=suppress_token_list,
                    num_beams=1,
                    do_sample=False,
                )
            
            predicted_ids_no_prompt = predicted_ids_asr[:, 4:]
            decoded_preds = tokenizer.batch_decode(
                predicted_ids_no_prompt, 
                skip_special_tokens=True
            )
            
            all_references_iemocap.extend(references)
            all_predictions_iemocap.extend(decoded_preds)
            wer_metric_iemocap.add_batch(predictions=decoded_preds, references=references)
    
    # ========================================================================
    # è©•ä¼° Common Voice (ASR)
    # ========================================================================
    print("\nè©•ä¼° Common Voice...")
    
    sot_id = tokenizer.bos_token_id
    en_id = tokenizer.convert_tokens_to_ids("<|en|>")
    transcribe_id = tokenizer.convert_tokens_to_ids("<|transcribe|>")
    notimestamps_id = tokenizer.convert_tokens_to_ids("<|notimestamps|>")
    cv_prompt_ids = [sot_id, en_id, transcribe_id, notimestamps_id]
    cv_prompt_len = len(cv_prompt_ids)
    
    with torch.no_grad():
        for batch in tqdm(val_loader_cv, desc="Common Voice é©—è­‰"):
            input_features = batch["input_features"].to(device)
            references = batch["original_texts"]
            batch_size = input_features.shape[0]
            
            decoder_prompt_cv = torch.tensor([cv_prompt_ids] * batch_size).to(device)
            
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                predicted_ids_asr = model.generate(
                    input_features=input_features,
                    decoder_input_ids=decoder_prompt_cv,
                    max_length=448,
                    use_cache=True,
                )
            
            predicted_ids_no_prompt = predicted_ids_asr[:, cv_prompt_len:]
            decoded_preds = tokenizer.batch_decode(
                predicted_ids_no_prompt, 
                skip_special_tokens=True
            )
            
            all_references_cv.extend(references)
            all_predictions_cv.extend(decoded_preds)
            wer_metric_cv.add_batch(predictions=decoded_preds, references=references)
    
    # ========================================================================
    # è¨ˆç®—æŒ‡æ¨™
    # ========================================================================
    
    # IEMOCAP æŒ‡æ¨™
    wer_iemocap = wer_metric_iemocap.compute()
    wa = accuracy_score(all_true_sle_ids, all_pred_sle_ids)
    uar = recall_score(
        all_true_sle_ids, all_pred_sle_ids,
        average='macro', labels=sle_id_list, zero_division=0
    )
    
    # è½‰æ›ç‚ºæƒ…ç·’æ¨™ç±¤
    all_true_emotions = [id_to_emotion.get(id, 'unknown') for id in all_true_sle_ids]
    all_pred_emotions = [id_to_emotion.get(id, 'unknown') for id in all_pred_sle_ids]
    
    emotions = ['neutral', 'happy', 'sad', 'angry']
    cm = confusion_matrix(
        all_true_emotions, all_pred_emotions,
        labels=emotions
    )
    
    per_class_recall = {}
    for i, emotion in enumerate(emotions):
        recall = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0
        per_class_recall[emotion] = float(recall)
    
    # Common Voice æŒ‡æ¨™
    wer_cv = wer_metric_cv.compute()
    
    # ========================================================================
    # é¡¯ç¤ºçµæœ
    # ========================================================================
    print(f"\n{'='*70}")
    print(f"Epoch {epoch} è©•ä¼°çµæœ")
    print(f"{'='*70}")
    
    print(f"\nIEMOCAP ({len(all_true_sle_ids)} æ¨£æœ¬):")
    print(f"  ASR - WER:  {wer_iemocap * 100:.2f}%")
    print(f"  SER - WA:   {wa * 100:.2f}%")
    print(f"  SER - UAR:  {uar * 100:.2f}% â­")
    
    print(f"\n  æ¯é¡åˆ¥å¬å›ç‡:")
    for emotion in emotions:
        recall_pct = per_class_recall[emotion] * 100
        print(f"    {emotion:8s}: {recall_pct:6.2f}%")
    
    print(f"\n  æ··æ·†çŸ©é™£:")
    print("           é æ¸¬:")
    print("         neutral  happy    sad    angry")
    for i, emotion in enumerate(emotions):
        print(f"{emotion:8s}  {cm[i, 0]:5d}  {cm[i, 1]:5d}  {cm[i, 2]:5d}  {cm[i, 3]:5d}")
    
    print(f"\nCommon Voice ({len(all_references_cv)} æ¨£æœ¬):")
    print(f"  ASR - WER:  {wer_cv * 100:.2f}%")
    
    print(f"{'='*70}\n")
    
    model.train()
    
    return {
        'iemocap': {
            'wer': float(wer_iemocap),
            'wa': float(wa),
            'uar': float(uar),
            'per_class_recall': per_class_recall,
            'confusion_matrix': cm.tolist(),
            'total_samples': len(all_true_sle_ids)
        },
        'common_voice': {
            'wer': float(wer_cv),
            'total_samples': len(all_references_cv)
        }
    }

# ============================================================================
# è¨“ç·´è¿´åœˆ
# ============================================================================
print("\n" + "="*70)
print("é–‹å§‹è¨“ç·´")
print("="*70 + "\n")

# ç”¨æ–¼ä¿å­˜æ‰€æœ‰ epoch çš„çµæœ
training_history = []

for epoch in range(EPOCHS):
    epoch_start_time = time.time()
    model.train()
    
    total_train_loss = 0
    total_loss_iemocap = 0
    total_loss_cv = 0
    num_batches = 0
    
    # å»ºç«‹è¨“ç·´è¿­ä»£å™¨
    if len(loader_iemocap) > len(loader_cv):
        train_progress_bar = tqdm(loader_iemocap, desc=f"Epoch {epoch+1}/{EPOCHS}")
        cv_iter = itertools.cycle(loader_cv)
        main_loader = loader_iemocap
        is_cv_main = False
    else:
        train_progress_bar = tqdm(loader_cv, desc=f"Epoch {epoch+1}/{EPOCHS}")
        iemocap_iter = itertools.cycle(loader_iemocap)
        main_loader = loader_cv
        is_cv_main = True

    optimizer.zero_grad()
    
    for i, main_batch in enumerate(train_progress_bar):
        try:
            # å–å¾—å…©å€‹è³‡æ–™é›†çš„æ‰¹æ¬¡
            if is_cv_main:
                batch_cv = main_batch
                batch_iemocap = next(iemocap_iter)
            else:
                batch_iemocap = main_batch
                batch_cv = next(cv_iter)

            # ======================================================
            # 1ï¸âƒ£ IEMOCAP loss
            # ======================================================
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs_iemocap = model(
                    input_features=batch_iemocap["input_features"].to(device),
                    labels=batch_iemocap["labels"].to(device),
                    decoder_input_ids=batch_iemocap["decoder_input_ids"].to(device)
                )
            loss_iemocap = outputs_iemocap.loss
            

            if torch.isnan(loss_iemocap) or torch.isinf(loss_iemocap):
                print(f"\n!! NaN/Inf Lossï¼Œè·³é !!")
                optimizer.zero_grad()
                continue
            (loss_iemocap / ACCUMULATION_STEPS).backward()

            # ======================================================
            # 2ï¸âƒ£ CV loss
            # ======================================================
            with autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs_cv = model(
                    input_features=batch_cv["input_features"].to(device),
                    labels=batch_cv["labels"].to(device),
                    decoder_input_ids=batch_cv["decoder_input_ids"].to(device)
                )
            loss_cv = outputs_cv.loss

            if torch.isnan(loss_cv) or torch.isinf(loss_cv):
                print(f"\n!! NaN/Inf CV Lossï¼Œè·³é !!")
                optimizer.zero_grad()
                continue
            (BETA * loss_cv / ACCUMULATION_STEPS).backward()

            # ======================================================
            # 3ï¸âƒ£ æ›´æ–°æ¬Šé‡
            # ======================================================
            if (i + 1) % ACCUMULATION_STEPS == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
                global_step_count += 1  # â† æå‰

                # âœ… æ›´æ–°çµ±è¨ˆï¼ˆloss ç”¨ loss_iemocap + BETA * loss_cvï¼‰
                total_loss_batch = loss_iemocap.item() + (BETA * loss_cv.item())
                total_train_loss += total_loss_batch
                total_loss_iemocap += loss_iemocap.item()
                total_loss_cv += loss_cv.item()
                num_batches += 1

                train_progress_bar.set_postfix({
                    "iem": f"{loss_iemocap.item():.4f}",
                    "cv": f"{loss_cv.item():.4f}",
                    "total": f"{total_loss_batch:.4f}",
                })
                
                # ========================================================
                # æ¯ N æ­¥å¿«é€Ÿé©—è­‰
                # ========================================================
                if global_step_count > 0 and (global_step_count % VALIDATION_STEPS == 0):
                    val_loss, val_uar, val_wa = quick_validation(
                        model, val_loader_iemocap, device, 
                        sle_token_ids, id_to_emotion, global_step_count
                    )
                    
                    # å¦‚æœ UAR æå‡ï¼Œä¿å­˜æª¢æŸ¥é»
                    if val_uar > best_uar:
                        print(f"ğŸ¯ æ–°çš„æœ€ä½³ UAR: {val_uar * 100:.2f}% (step {global_step_count})")
                        print(f"   ä¿å­˜è‡¨æ™‚æª¢æŸ¥é»...")
                        
                        checkpoint_dir = MODEL_SAVE_PATH / f"checkpoint_step_{global_step_count}"
                        checkpoint_dir.mkdir(exist_ok=True)
                        
                        model.save_pretrained(checkpoint_dir)
                        tokenizer.save_pretrained(checkpoint_dir)
                        
                        # ä¿å­˜æª¢æŸ¥é»è³‡è¨Š
                        checkpoint_info = {
                            'step': global_step_count,
                            'epoch': epoch + 1,
                            'uar': float(val_uar),
                            'wa': float(val_wa),
                            'loss': float(val_loss),
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        checkpoint_file = checkpoint_dir / "checkpoint_info.json"
                        with open(checkpoint_file, 'w', encoding='utf-8') as f:
                            json.dump(checkpoint_info, f, indent=2, ensure_ascii=False)
                        
                        print("âœ“ æª¢æŸ¥é»å·²ä¿å­˜")
                    
                    gc.collect()
                    torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"\nè¨“ç·´æ‰¹æ¬¡éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            optimizer.zero_grad()
            continue

    # ========================================================================
    # Epoch çµæŸï¼šè¨ˆç®—è¨“ç·´æå¤±ä¸¦é€²è¡Œå®Œæ•´è©•ä¼°
    # ========================================================================
    avg_train_loss = total_train_loss / max(num_batches, 1)
    avg_iem_loss = total_loss_iemocap / max(num_batches, 1)
    avg_cv_loss = total_loss_cv / max(num_batches, 1)
    epoch_time = (time.time() - epoch_start_time) / 60
    
    # å®Œæ•´è©•ä¼°
    eval_results = evaluate_model(
        model, val_loader_iemocap, val_loader_cv, tokenizer, device,
        sle_token_ids, wle_token_ids, id_to_emotion, epoch + 1
    )
    
    current_uar = eval_results['iemocap']['uar']
    current_wa = eval_results['iemocap']['wa']
    current_wer_iem = eval_results['iemocap']['wer']
    current_wer_cv = eval_results['common_voice']['wer']
    
    # æ›´æ–°å­¸ç¿’ç‡æ’ç¨‹å™¨ï¼ˆä½¿ç”¨ UARï¼‰
    plateau_scheduler.step(current_uar)
    
    # ä¿å­˜ epoch çµæœ
    epoch_summary = {
        'epoch': epoch + 1,
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'training': {
            'total_loss': float(avg_train_loss),
            'iemocap_loss': float(avg_iem_loss),
            'cv_loss': float(avg_cv_loss),
            'time_minutes': float(epoch_time)
        },
        'validation': eval_results,
        'learning_rate': optimizer.param_groups[0]['lr']
    }
    training_history.append(epoch_summary)
    
    # ä¿å­˜è¨“ç·´æ­·å²
    history_file = MODEL_SAVE_PATH / "training_history.json"
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(training_history, f, indent=2, ensure_ascii=False)
    
    # ========================================================================
    # æ ¹æ“š UAR ä¿å­˜æœ€ä½³æ¨¡å‹
    # ========================================================================
    if current_uar > best_uar:
        best_uar = current_uar
        
        print(f"\nğŸ¯ æ–°çš„æœ€ä½³ UAR: {best_uar * 100:.2f}%ï¼")
        print(f"   ä¿å­˜æ¨¡å‹è‡³ {MODEL_SAVE_PATH}...")
        
        model.save_pretrained(MODEL_SAVE_PATH)
        tokenizer.save_pretrained(MODEL_SAVE_PATH)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹è³‡è¨Š
        best_model_info = {
            'epoch': epoch + 1,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'uar': float(best_uar),
            'wa': float(current_wa),
            'wer_iemocap': float(current_wer_iem),
            'wer_cv': float(current_wer_cv),
            'per_class_recall': eval_results['iemocap']['per_class_recall'],
            'confusion_matrix': eval_results['iemocap']['confusion_matrix']
        }
        
        best_model_file = MODEL_SAVE_PATH / "best_model_info.json"
        with open(best_model_file, 'w', encoding='utf-8') as f:
            json.dump(best_model_info, f, indent=2, ensure_ascii=False)
        
        print("âœ“ æ¨¡å‹å·²ä¿å­˜")
    
    # ========================================================================
    # Epoch ç¸½çµ
    # ========================================================================
    print("\n" + "="*70)
    print(f"Epoch {epoch+1}/{EPOCHS} å®Œæˆ ({epoch_time:.1f} åˆ†é˜)")
    print("="*70)
    print(f"è¨“ç·´æå¤±:")
    print(f"  ç¸½ Loss:      {avg_train_loss:.4f}")
    print(f"  IEMOCAP Loss: {avg_iem_loss:.4f}")
    print(f"  CV Loss:      {avg_cv_loss:.4f}")
    print(f"\né©—è­‰çµæœ:")
    print(f"  UAR:          {current_uar * 100:.2f}% {'â­ (æœ€ä½³)' if current_uar == best_uar else ''}")
    print(f"  WA:           {current_wa * 100:.2f}%")
    print(f"  WER (IEMOCAP): {current_wer_iem * 100:.2f}%")
    print(f"  WER (CV):     {current_wer_cv * 100:.2f}%")
    print(f"\næœ€ä½³ UAR:      {best_uar * 100:.2f}%")
    print("="*70 + "\n")
    
    gc.collect()
    torch.cuda.empty_cache()

# ============================================================================
# è¨“ç·´å®Œæˆ
# ============================================================================
print("\n" + "="*70)
print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
print("="*70)
print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
print(f"æœ€ä½³ UAR: {best_uar * 100:.2f}%")
print(f"è¨“ç·´æ­·å²å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH / 'training_history.json'}")
print(f"æœ€ä½³æ¨¡å‹è³‡è¨Šå·²ä¿å­˜è‡³: {MODEL_SAVE_PATH / 'best_model_info.json'}")
print("="*70)
